# ü¶ô LLaMA 3.2 Fine-tuned with GRPO for Financial & Mathematical Reasoning

<a href="https://colab.research.google.com/github/iamfaham/llama-3.2-GRPO-finetuned/blob/main/Llama_3_4b_GRPO.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

This project demonstrates fine-tuning **LLaMA 3.2** models using **GRPO (Generalized Reward Preference Optimization)** for enhanced financial and mathematical reasoning capabilities. The implementation uses **Unsloth** for efficient training and supports both 3B and 8B parameter variants.

## üéØ Project Overview

This notebook implements a complete pipeline for:

- **Supervised Fine-Tuning (SFT)** for format enforcement
- **GRPO training** for reward-based optimization
- **Multi-dataset training** on financial and mathematical reasoning tasks
- **Comprehensive evaluation** with visualization

The model is trained to provide step-by-step reasoning in a structured format:

```
<start_contemplating>
[detailed reasoning steps]
<end_contemplating>
<SOLUTION>[final answer]</SOLUTION>
```

## üöÄ Key Features

- ‚úÖ **Multi-Model Support**: LLaMA 3.2-3B, LLaMA 3.2-8B, and Qwen3-4B-Base
- ‚úÖ **Advanced Training**: Two-stage training with SFT ‚Üí GRPO
- ‚úÖ **Multi-Dataset Learning**: GSM8K, AQuA-RAT, FinQA, and TAT-QA
- ‚úÖ **Efficient Training**: Uses Unsloth for optimized performance
- ‚úÖ **LoRA Adaptation**: Parameter-efficient fine-tuning
- ‚úÖ **Reward Engineering**: Custom reward functions for format and accuracy
- ‚úÖ **Comprehensive Evaluation**: Accuracy and format compliance metrics
- ‚úÖ **Visualization**: Training curves and performance comparisons

## üìä Datasets

The model is trained on a combination of mathematical and financial reasoning datasets:

| Dataset      | Description                         | Size        | Domain      |
| ------------ | ----------------------------------- | ----------- | ----------- |
| **GSM8K**    | Grade school math word problems     | ~7.5K train | Mathematics |
| **AQuA-RAT** | Algebraic reasoning with rationales | ~97K train  | Mathematics |
| **FinQA**    | Financial question answering        | Variable    | Finance     |
| **TAT-QA**   | Tabular and textual reasoning       | Variable    | Finance     |

**Total Training Examples**: ~104K (filtered to ~94K based on token length)

## üèóÔ∏è Architecture & Training

### Model Configuration

- **Base Models**:
  - `unsloth/Llama-3.2-3B` (default)
  - `unsloth/Llama-3.2-8B`
  - `unsloth/Qwen3-4B-Base`
- **LoRA Parameters**:
  - Rank: 16
  - Alpha: 32
  - Dropout: 0.05
  - Target modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

### Training Pipeline

#### Stage 1: Supervised Fine-Tuning (SFT)

- **Purpose**: Format enforcement and initial adaptation
- **Epochs**: 2
- **Subset Size**: 600 examples
- **Learning Rate**: 2e-4
- **Batch Size**: 1 (with gradient accumulation: 2)

#### Stage 2: GRPO Training

- **Purpose**: Reward-based optimization for accuracy and format
- **Steps**: 300
- **Learning Rate**: 1e-5
- **Batch Size**: 1 (with gradient accumulation: 2)
- **Dataset Size**: 2,000 examples

### Reward Functions

1. **Format Exact Match** (+3): Both contemplation and solution tags present
2. **Format Approximate** (+1): Partial format compliance
3. **Answer Accuracy** (+5 exact, +2 within 10%): Numerical correctness
4. **Reasoning Bonus** (+1): Non-empty reasoning content

## üõ†Ô∏è Installation & Setup

### Requirements

```bash
pip install unsloth unsloth-zoo transformers trl accelerate peft datasets
pip install einops bitsandbytes evaluate fugashi ipywidgets
pip install vllm==0.8.5.post1  # Optional for faster inference
```

### Hardware Requirements

- **GPU**: NVIDIA GPU with CUDA support (tested on Tesla T4)
- **Memory**: 15GB+ GPU VRAM recommended
- **Platform**: Google Colab Pro/Pro+ recommended

## üöÄ Quick Start

1. **Open in Colab**: Click the "Open in Colab" badge above
2. **Run Setup Cell**: Execute the first cell to install dependencies
3. **Configure Model**: Choose between LLaMA variants or Qwen
4. **Train Model**: Run all cells sequentially
5. **Evaluate**: Review performance metrics and sample outputs

### Key Configuration Options

```python
# Model Selection
USE_QWEN = False  # True for Qwen3-4B-Base
LLAMA_VARIANT = "unsloth/Llama-3.2-3B"  # or "unsloth/Llama-3.2-8B"

# Training Parameters
SFT_EPOCHS = 2
GRPO_STEPS = 300
MIXED_PRECISION = "fp16"  # "bf16", "fp16", or "fp32"
MAX_SEQ_LEN = 2048
```

## üìà Performance Results

The model shows improved performance after GRPO training:

| Metric            | Baseline | GRPO  | Improvement |
| ----------------- | -------- | ----- | ----------- |
| **Accuracy**      | 48.0%    | 42.5% | -5.5%       |
| **Format Exact**  | 92.5%    | 95.0% | +2.5%       |
| **Format Approx** | 92.5%    | 95.5% | +3.0%       |

_Note: While accuracy decreased slightly, format compliance improved significantly, showing the trade-off in GRPO optimization._

## üíæ Model Outputs

The training process generates several important artifacts:

- `./sft_out/`: SFT checkpoint and tokenizer
- `./grpo_out/`: Final GRPO model and tokenizer
- `./lora_adapter/`: LoRA adapter weights
- Training visualizations and evaluation metrics

## üî¨ Usage Examples

### Inference with Fine-tuned Model

```python
def chat_infer(question: str):
    prompt = to_chat_text([
        {"role":"system","content":"You are a financial and math reasoning assistant. Use step-by-step reasoning."},
        {"role":"user","content":question},
    ])
    return run_model_inference(model_grpo, tokenizer_grpo, [prompt])

# Example usage
result = chat_infer("If a stock grows 12% annually, what is its value after 5 years starting from $500?")
```

### Expected Output Format

```
<start_contemplating>
To find the value after 5 years with 12% annual growth:
- Initial value: $500
- Growth rate: 12% = 0.12
- Formula: Final Value = Initial √ó (1 + rate)^years
- Calculation: $500 √ó (1.12)^5 = $500 √ó 1.7623 = $881.15
<end_contemplating>
<SOLUTION>$881.15</SOLUTION>
```

## üß™ Evaluation & Visualization

The notebook includes comprehensive evaluation tools:

- **Accuracy Metrics**: Numerical correctness assessment
- **Format Compliance**: Structure adherence scoring
- **Comparative Analysis**: Before/after GRPO performance
- **Sample Outputs**: Qualitative comparison examples
- **Training Curves**: Reward progression visualization

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

- Additional reward functions
- Support for larger models
- Extended dataset integration
- Performance optimization

## üìù License

This project is open source. Please check individual dataset licenses for usage restrictions.

## üôè Acknowledgments

- **Unsloth**: For efficient LLaMA training framework
- **TRL**: For GRPO implementation
- **Hugging Face**: For model hosting and transformers library
- **Dataset Creators**: GSM8K, AQuA-RAT, FinQA, TAT-QA teams

## üìö References

- [Unsloth Documentation](https://github.com/unslothai/unsloth)
- [TRL Library](https://github.com/huggingface/trl)
- [LLaMA 3.2 Paper](https://ai.meta.com/research/publications/llama-3-2/)
- [GRPO: Group Relative Policy Optimization](https://arxiv.org/abs/2402.03300)

---

**Happy fine-tuning! üöÄ**
